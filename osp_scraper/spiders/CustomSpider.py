# -*- coding: utf-8 -*-

from . import OSPSpider, ALLOWED_FILE_TYPES
from ..filterware import Filter
from ..items import PageItem

class CustomSpider(OSPSpider):
    custom_settings = {
        **OSPSpider.custom_settings,
        'ROBOTSTXT_OBEY': False
    }

    @classmethod
    def from_crawler(cls, crawler, *args, **kwargs):
        spider = super().from_crawler(crawler, *args, **kwargs)
        spider.allowed_file_types = ALLOWED_FILE_TYPES
        spider.filters = [Filter.compile('allow')]
        return spider

    def start_requests(self):
        """
        Override in subclass for each site when necessary.  Overriding
        start_requests is not necessary when implementing parse.

        By default, sets both the 'depth' and 'hops_from_seed' meta content to
        0.  This can be useful when making calls to parse_for_files directly out
        of parse.
        """
        for request in super().start_requests():
            request.meta['depth'] = 0
            request.meta['hops_from_seed'] = 0
            yield request

    def parse(self, response):
        """
        Implement in subclass for each site when necessary.  Implementing parse
        is not necessary when implementing start_requests.
        """
        raise NotImplementedError

    def extract_links(self, response):
        """
        Generate (url, source_anchor) tuples extracted from the page

        Implement in subclass for each site as necessary.
        """
        return
        yield

    def save_item(self, response):
        """Yield a PageItem with all necessary metadata for the response.
        """
        yield PageItem(
            url=response.url,
            content=response.body,
            headers=response.headers,
            status=response.status,
            source_url=response.meta['source_url'],
            source_anchor=self.clean_whitespace(response.meta['source_anchor']),
            depth=response.meta['depth'],
            hops_from_seed=response.meta['hops_from_seed']
        )

    def parse_for_files(self, response):
        """Yield a PageItem for the response and generate request for files.

        DEPRECATED with removal of WarcFilePipeline.  To avoid multiple pages
        for courses and unnecessary files, use save_item instead.

        Generate PageItems with all necessary metadata and generate requests
        for file urls generated by extract_links.

        If you set the require_files flag on response.meta to True, a PageItem
        will not be yielded.
        """
        for url, anchor in self.extract_links(response):
            meta = {
                'source_url': response.url,
                'source_anchor': self.clean_whitespace(anchor),
                'depth': response.meta['depth'] + 1,
                'hops_from_seed': response.meta['hops_from_seed'] + 1,
            }

            yield scrapy.Request(
                response.urljoin(url),
                meta=meta
            )

        if response.meta.get('require_files'):
            return

        yield PageItem(
            url=response.url,
            content=response.body,
            headers=response.headers,
            status=response.status,
            source_url=response.meta['source_url'],
            source_anchor=self.clean_whitespace(response.meta['source_anchor']),
            depth=response.meta['depth'],
            hops_from_seed=response.meta['hops_from_seed']
        )

    def clean_whitespace(self, s):
        """
        Condenses all consecutive whitespace in a string to a single
        space and removes leading and trailing whitespace.
        """
        return " ".join(s.split())
